<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmlns:epf="http://www.eclipse.org/epf" epf:version="1.5.1" xmlns:rmc="http://www.ibm.com/rmc" rmc:version="7.5.1" xmi:id="-9gUpkUYqONF3x8UWwAO_zw" name="failure_analysis_rpt_creation,_0jhR0MlgEdmt3adZL5Dmdw" guid="-9gUpkUYqONF3x8UWwAO_zw" changeDate="2006-09-29T01:52:52.000-0700" version="1.0.0">
  <mainDescription>&lt;h3>&#xD;
    Introduction&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    During testing, you will encounter failures related to the execution of your tests in different forms, such as code&#xD;
    defects, user errors, program malfunctions, and general problems. This&amp;nbsp;concept discusses some ways to conduct&#xD;
    failure analysis and then to report your findings.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Failure Analysis&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    After you have run your tests, it is good practice to identify inputs for review of the results of the testing effort.&#xD;
    Some likely sources are defects that occurred during the execution of test scripts, change request metrics, and test&#xD;
    log details.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Running test scripts results in errors of different kinds such as uncovered defects, unexpected behavior, or general&#xD;
    failure of the test script to run properly. When you run test scripts, one of the most important things to do is to&#xD;
    identify causes and effects of failure. It is important to differentiate failures in the system under test&amp;nbsp;from&#xD;
    those related to the tests themselves.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Change request metrics are useful in analyzing and correcting failures in the testing. Select metrics that will&#xD;
    facilitate creation of incident reports from a collection of change requests.&#xD;
&lt;/p>&#xD;
&lt;p>&#xD;
    Change request metrics that you may find useful in your failure analysis include:&#xD;
&lt;/p>&#xD;
&lt;ul>&#xD;
    &lt;li>&#xD;
        test coverage&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        priority&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        impact&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        defect trends&#xD;
    &lt;/li>&#xD;
    &lt;li>&#xD;
        density&#xD;
    &lt;/li>&#xD;
&lt;/ul>&#xD;
&lt;p>&#xD;
    Finally, one of the most critical sources of your failure analysis is the test log. Start by gathering the test log's&#xD;
    output during the implementation and execution of the tests. Relevant logs might come from many sources; they might be&#xD;
    captured by the tools you use (both test execution and diagnostic tools), generated by custom-written routines your&#xD;
    team has developed, output from the target test items themselves, and recorded manually be the tester. Gather all of&#xD;
    the available test log sources and examine their content. Check that all the scheduled testing executed to completion,&#xD;
    and that all the needed tests&amp;nbsp;have been scheduled.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Self-Documenting Tests&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    For automated tests it is a best practice for the test itself to examine the results and clearly report itself as&#xD;
    passing or failing. This provides the most efficient way to run tests such that whole suites of tests can be run with&#xD;
    each test in turn determining whether it has passed or failed without the need for human intervention. When authoring&#xD;
    self-documenting tests, take extra care to ensure that the analysis of the results considers all possibilities.&#xD;
&lt;/p>&#xD;
&lt;h3>&#xD;
    Recording Your Findings&#xD;
&lt;/h3>&#xD;
&lt;p>&#xD;
    Once you have conducted your failure analysis, you may decide to formalize the results of this analysis by recording&#xD;
    your findings in a report. There are several factors that go into deciding whether to record your failure analysis in a&#xD;
    report. Some of the key factors include: level of testing formality, complexity of the testing effort, and the need to&#xD;
    communicate the testing results to the entire development team. In less formal environments, it may be sufficient to&#xD;
    record your failure analysis in&amp;nbsp;a test evaluation summary.&#xD;
&lt;/p></mainDescription>
</org.eclipse.epf.uma:ContentDescription>
