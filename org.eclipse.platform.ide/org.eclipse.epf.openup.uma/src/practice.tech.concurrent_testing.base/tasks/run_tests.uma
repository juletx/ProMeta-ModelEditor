<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:TaskDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmi:id="_NrbRUqeqEdmKDbQuyzCoqQ" name="run_tests,_0jVEkMlgEdmt3adZL5Dmdw" guid="_NrbRUqeqEdmKDbQuyzCoqQ" changeDate="2007-12-06T11:34:58.000+0100" version="1.0.0" keyConsiderations="&lt;ul>&#xD;&#xA;    &lt;li>&#xD;&#xA;        Run all tests as frequently as possible. Ideally, run all test scripts against each build deployed to the test&#xD;&#xA;        environment. If this is impractical, run regression tests for existing functionality, and&amp;nbsp;focus the test cycle&#xD;&#xA;        on work items completed in the new build.&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        Even test scripts that are expected to fail provide valuable feedback. However, once a test script is passing, it&#xD;&#xA;        should not fail&amp;nbsp;against subsequent builds of the solution.&#xD;&#xA;    &lt;/li>&#xD;&#xA;&lt;/ul>" longPresentationName="run_tests,_0jVEkMlgEdmt3adZL5Dmdw" purpose="To provide feedback to the team about how well a build satisfies the requirements.">
  <sections xmi:id="_xVhnwKRLEdyLP-jEVj8Kyw" name="Review work items completed in the build" guid="_xVhnwKRLEdyLP-jEVj8Kyw" variabilityBasedOnElement="_xVhnwKRLEdyLP-jEVj8Kyw" sectionDescription="Review work items that were integrated into the build since the last test cycle. Focus on identifying any previously&#xD;&#xA;unimplemented or failing requirements are now expected to meet the conditions of satisfaction."/>
  <sections xmi:id="_1L1yAKRLEdyLP-jEVj8Kyw" name="Select Test Scripts" guid="_1L1yAKRLEdyLP-jEVj8Kyw" variabilityBasedOnElement="_1L1yAKRLEdyLP-jEVj8Kyw" sectionDescription="&lt;p>&#xD;&#xA;    Select test scripts related to work items completed in the build.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    Ideally, each test cycle should execute all test scripts, but some types of tests are too time-consuming to include in&#xD;&#xA;    each test cycle. For manual or time-intensive tests, include test scripts that will provide the most useful feedback&#xD;&#xA;    about the maturing solution based on the objectives of the iteration.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    Plan with test suites to&amp;nbsp;simplify the process of selecting tests for each build (see &lt;a&#xD;&#xA;    class=&quot;elementLinkWithType&quot; href=&quot;./../../core.tech.common.extend_supp/guidances/guidelines/test_suite_D54EEBED.html&quot;&#xD;&#xA;    guid=&quot;_0aDz0MlgEdmt3adZL5Dmdw&quot;>Guideline: Test Suite&lt;/a>).&#xD;&#xA;&lt;/p>"/>
  <sections xmi:id="_gV408KuSEdmhFZtkg1nakg" name="Execute Test Scripts against the build" guid="_gV408KuSEdmhFZtkg1nakg" variabilityBasedOnElement="_gV408KuSEdmhFZtkg1nakg" sectionDescription="&lt;p>&#xD;&#xA;    Run the tests using the step-by-step procedure in the &lt;a class=&quot;elementLink&quot;&#xD;&#xA;    href=&quot;./../../core.tech.common.extend_supp/workproducts/test_script_39A30BA2.html&quot; guid=&quot;_0ZfMEMlgEdmt3adZL5Dmdw&quot;>Test&#xD;&#xA;    Script&lt;/a>.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    For automated test scripts, initiate the test execution.&amp;nbsp;Automated test scripts should run in suites in the&#xD;&#xA;    correct sequence, and collect results in the Test Log.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    To execute a manual test script, establish its preconditions, perform the steps while logging results in the &lt;a&#xD;&#xA;    class=&quot;elementLink&quot; href=&quot;./../../core.tech.common.extend_supp/workproducts/test_log_CBA2FDF4.html&quot;&#xD;&#xA;    guid=&quot;_0ZlSsMlgEdmt3adZL5Dmdw&quot;>Test Log&lt;/a>, and perform any teardown steps.&#xD;&#xA;&lt;/p>"/>
  <sections xmi:id="_sQaC4DO2EduqsLmIADMQ9g" name="Analyze and communicate test results" guid="_sQaC4DO2EduqsLmIADMQ9g" variabilityBasedOnElement="_sQaC4DO2EduqsLmIADMQ9g" sectionDescription="&lt;p>&#xD;&#xA;    Post the test results in a conspicuous place that is accessible to the entire team, such as a white board or Wiki.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    For each failing test script, analyze the Test Log to identify the cause of the test failure. Begin with failing tests&#xD;&#xA;    that you expected to begin passing against this build, which may indicate newly delivered work items that do not meet&#xD;&#xA;    the conditions of satisfaction. Then review previously passing test scripts that are now failing, which may indicate&#xD;&#xA;    regressive issues in the build.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;ul>&#xD;&#xA;    &lt;li>&#xD;&#xA;        If a test failed because the solution does not meet the conditions of satisfaction for the test case, log the issue&#xD;&#xA;        in the Work Items List. In the work item, clearly identify the observed behavior, the expected behavior, and steps&#xD;&#xA;        to repeat the issue. Note which failing test initially discovered the issue.&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        If a test failed because of a change in the system (such as a user-interface change), but the implementation still&#xD;&#xA;        meets the conditions of satisfaction in the test case, update the test script to pass with the new implementation.&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        If a test failed because the test script is incorrect (a false negative result) or passed when it was expected to&#xD;&#xA;        fail (a false positive result), update the test script to correctly implement the conditions of satisfaction in the&#xD;&#xA;        test case. If the test case for a requirement is invalid, create a request change to modify the conditions of&#xD;&#xA;        satisfaction for the requirement.&amp;nbsp;&#xD;&#xA;    &lt;/li>&#xD;&#xA;&lt;/ul>&#xD;&#xA;&lt;p>&#xD;&#xA;    It's best to update test scripts as quickly and continuously as possible. If the change to the test script is trivial,&#xD;&#xA;    update the test while analyzing the test results. If the change is a non-trivial task, submit it to the Work Items List&#xD;&#xA;    so it can be prioritized against other tasks.&#xD;&#xA;&lt;/p>"/>
  <sections xmi:id="_3t6oADO2EduqsLmIADMQ9g" name="Provide feedback to the team" guid="_3t6oADO2EduqsLmIADMQ9g" variabilityBasedOnElement="_3t6oADO2EduqsLmIADMQ9g" sectionDescription="&lt;p>&#xD;&#xA;    Summarize and provide feedback to the team about how well the build satisfies the requirements planned to the&#xD;&#xA;    iteration. Focus on measuring progress in terms of passing tests.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    Explain the results for the test cycle&amp;nbsp;in the context of overall trends:&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;ul>&#xD;&#xA;    &lt;li>&#xD;&#xA;        How many tests were selected for the build, and what&amp;nbsp;are their statuses (pass, fail, blocked, not run, etc.)?&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        How many issues were added to the Work Items List, and what are their statuses and severities?&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        For test scripts that were blocked or skipped, what are the main reasons (such as known issues)?&#xD;&#xA;    &lt;/li>&#xD;&#xA;&lt;/ul>"/>
</org.eclipse.epf.uma:TaskDescription>
