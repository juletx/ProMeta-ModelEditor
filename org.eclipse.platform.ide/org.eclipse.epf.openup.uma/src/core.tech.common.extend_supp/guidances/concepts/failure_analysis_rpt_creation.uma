<?xml version="1.0" encoding="UTF-8"?>
<org.eclipse.epf.uma:ContentDescription xmi:version="2.0" xmlns:xmi="http://www.omg.org/XMI" xmlns:org.eclipse.epf.uma="http://www.eclipse.org/epf/uma/1.0.6/uma.ecore" xmi:id="-9gUpkUYqONF3x8UWwAO_zw" name="failure_analysis_rpt_creation,_0jhR0MlgEdmt3adZL5Dmdw" guid="-9gUpkUYqONF3x8UWwAO_zw" changeDate="2006-09-29T10:52:52.000+0200" version="1.0.0" mainDescription="&lt;h3>&#xD;&#xA;    Introduction&#xD;&#xA;&lt;/h3>&#xD;&#xA;&lt;p>&#xD;&#xA;    During testing, you will encounter failures related to the execution of your tests in different forms, such as code&#xD;&#xA;    defects, user errors, program malfunctions, and general problems. This&amp;nbsp;concept discusses some ways to conduct&#xD;&#xA;    failure analysis and then to report your findings.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;h3>&#xD;&#xA;    Failure Analysis&#xD;&#xA;&lt;/h3>&#xD;&#xA;&lt;p>&#xD;&#xA;    After you have run your tests, it is good practice to identify inputs for review of the results of the testing effort.&#xD;&#xA;    Some likely sources are defects that occurred during the execution of test scripts, change request metrics, and test&#xD;&#xA;    log details.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    Running test scripts results in errors of different kinds such as uncovered defects, unexpected behavior, or general&#xD;&#xA;    failure of the test script to run properly. When you run test scripts, one of the most important things to do is to&#xD;&#xA;    identify causes and effects of failure. It is important to differentiate failures in the system under test&amp;nbsp;from&#xD;&#xA;    those related to the tests themselves.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    Change request metrics are useful in analyzing and correcting failures in the testing. Select metrics that will&#xD;&#xA;    facilitate creation of incident reports from a collection of change requests.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;p>&#xD;&#xA;    Change request metrics that you may find useful in your failure analysis include:&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;ul>&#xD;&#xA;    &lt;li>&#xD;&#xA;        test coverage&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        priority&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        impact&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        defect trends&#xD;&#xA;    &lt;/li>&#xD;&#xA;    &lt;li>&#xD;&#xA;        density&#xD;&#xA;    &lt;/li>&#xD;&#xA;&lt;/ul>&#xD;&#xA;&lt;p>&#xD;&#xA;    Finally, one of the most critical sources of your failure analysis is the test log. Start by gathering the test log's&#xD;&#xA;    output during the implementation and execution of the tests. Relevant logs might come from many sources; they might be&#xD;&#xA;    captured by the tools you use (both test execution and diagnostic tools), generated by custom-written routines your&#xD;&#xA;    team has developed, output from the target test items themselves, and recorded manually be the tester. Gather all of&#xD;&#xA;    the available test log sources and examine their content. Check that all the scheduled testing executed to completion,&#xD;&#xA;    and that all the needed tests&amp;nbsp;have been scheduled.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;h3>&#xD;&#xA;    Self-Documenting Tests&#xD;&#xA;&lt;/h3>&#xD;&#xA;&lt;p>&#xD;&#xA;    For automated tests it is a best practice for the test itself to examine the results and clearly report itself as&#xD;&#xA;    passing or failing. This provides the most efficient way to run tests such that whole suites of tests can be run with&#xD;&#xA;    each test in turn determining whether it has passed or failed without the need for human intervention. When authoring&#xD;&#xA;    self-documenting tests, take extra care to ensure that the analysis of the results considers all possibilities.&#xD;&#xA;&lt;/p>&#xD;&#xA;&lt;h3>&#xD;&#xA;    Recording Your Findings&#xD;&#xA;&lt;/h3>&#xD;&#xA;&lt;p>&#xD;&#xA;    Once you have conducted your failure analysis, you may decide to formalize the results of this analysis by recording&#xD;&#xA;    your findings in a report. There are several factors that go into deciding whether to record your failure analysis in a&#xD;&#xA;    report. Some of the key factors include: level of testing formality, complexity of the testing effort, and the need to&#xD;&#xA;    communicate the testing results to the entire development team. In less formal environments, it may be sufficient to&#xD;&#xA;    record your failure analysis in&amp;nbsp;a test evaluation summary.&#xD;&#xA;&lt;/p>" longPresentationName="failure_analysis_rpt_creation,_0jhR0MlgEdmt3adZL5Dmdw"/>
